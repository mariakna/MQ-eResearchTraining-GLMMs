---
title: "Data analysis using (generalised) linear mixed effects models in R"
author: "Maria Korochkina"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 5
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    theme: cerulean
fontsize: 16pt
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Load required packages

```{r, results = "hide"}
rm(list=ls())

library("openxlsx")
library("Rmisc")
library("tidyverse")
library("lme4")
library("car")
library("MASS")
library("scales")
library("lmerTest")
library("sjmisc")
library("sjPlot")
library("ggsignif")
library("RColorBrewer")
library("rms")
```

# Load data

Data that we will be using today is publicly available [here](https://github.com/mariakna/MQ-eResearchTraning--G-LMEmodels).

Download the file *pwiExp.xlsx*, save it in a directory of your choice, make this directory your working directory by using the command `setwd()` and load the file as shown below:

```{r}
data <- read.xlsx("pwiExp.xlsx", sheet = 1)
```

## Data description

This file contains data from a picture-word interference task. In this task, German native speakers named (in German) pictures of familiar concepts (e.g., elephant) with superimposed written word distractors. The distractors were either semantically related (e.g., 'cat') or unrelated (e.g.'hammer') to the depicted concepts. The distractors were (a) German words, (b) novel words that the participants had learned earlier, or (c) unknown pseudowords. 

<center>
![FigName](pwi.png){width=300px}
</center>

The data we just loaded contains lots of information, but the relevant things for us today are:

* `Subj`: Participant ID (there were 60 participants in total)
* `List`: Which list the participant received (1-5)
* `ItemType`: Experimental or filler (this file only contains experimental items)
* `Cond`: Condition. This a factor with 5 levels: `SemRelG` (semantically related German), `SemUnRelG` (semantically unrelated German), `SemRelN` (semantically related novel), `SemUnRelN` (semantically unrelated novel), `Baseline` (unknown pseudowords as distractors)
* `TargetAnswer`: correct picture name (in German)
* `Response`: participant's response
* `Accuracy`: a factor with 2 levels, `0` = incorrect responses, `1` = correct responses
* `RT`: response times

<style>
div.blue{background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

**Hypotheses 1 & 2:**

* Compared to semantically unrelated German distractors, German distractors from the same semantic category will slow picture naming response times
* Compared to unknown pseudowords, German semantically unrelated distractors will slow picture naming response times

</div>
<p>  </p>

Because our first two hypotheses only concern German and unknown pseudoword distractors, we will restrict the dataset:

```{r}
data2 <- data %>%
  filter(Cond == "SemRelG" | Cond == "SemUnRelG" | Cond == "Baseline")
```

Next, adjust factor levels for the independent variable and random terms:

```{r}
data2$Cond <- factor(data2$Cond)
levels(data2$Cond)

data2$Subj <- factor(data2$Subj)
data2$TargetAnswer <- factor(data2$TargetAnswer)
```

# Inspect data

This step is essential and should not be neglected as it allows you to ensure that your data looks as it should. You can do it in any way you want. I often used the function `xtabs()`, for example:

```{r}
head(xtabs(~ Subj + List,data2)) 
head(xtabs(~ Subj + Cond,data2)) 
xtabs(~ ItemType + Cond,data2) 
xtabs(~ List + Cond,data2) 
```

# Prepare data for analysis

## Remove incorrect responses

We will run our analysis on correct responses only so we first need to check accuracy and exclude observations with incorrect responses.

Let's look at overall accuracy first:

```{r}
ftable(data2$Accuracy)
```

And now accuracy per condition:

```{r}
(Accuracy <- summarySEwithin(data2, measurevar = "Accuracy", withinvars = "Cond",
                       idvar = "Subj", na.rm = FALSE, conf.interval = .95))
```

**Note:** The function `summarySEwithin` is a good (although not yet widely accepted) method to summarise data for experiments with a within-subjects design. The standard SE formula

$$
\sigma_{\overline{x}} = \frac{s}{\sqrt{n}}
$$

does  not take into  account  that there are multiple  data  points  from  each  participant  and, instead,  summarises  the  variability  across  all  observations  as  if  they  were  independent. Similarly, aggregating the data and calculating averages per participant per condition would conflate variability associated with each participantâ€™s performance and random error with the variability  associated  with  the  experimental  manipulation. One method  to  compute  standard error while also disentangling these two sources of variability was described in [Morey (2008)](http://pcl.missouri.edu/sites/default/files/morey.2008.pdf) and later implemented in the [Rmisc package](http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/) in R by Ryan Hope. 

Luckily, the participants are at ceiling! We can now restrict the data to correct responses:

```{r}
dataCorr <- data2 %>%
  filter(Accuracy == 1)
```

## Data trimming

There are many ways to do it, and it often depends on your research question and your knowledge about the processes you are studying. For example, in word production, it often makes sense to remove data points with super fast and super slow responses. 

I quite like the approach of keeping as many observations as you can and trying to model them so I often start with inspecting the data distribution and only removing those data points that clearly stand out:

```{r}
plot(density(dataCorr$RT))
```

Based on the data distribution and what we know about picture naming, we remove responses faster than 300ms and slower than 2000ms. 

```{r}
dataCorr2 <- dataCorr %>%
 filter(RT > 300 & RT < 2000) # 12 obs removed
```

# Contrast coding

<style>
div.blue{background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

**Hypotheses 1 & 2:**

- Compared to semantically unrelated German distractors, German distractors from the same semantic category will slow picture naming response times
- Compared to unknown pseudowords, German semantically unrelated distractors will slow picture naming response times

</div>
<p>  </p>

How can we test these hypotheses?

1. We could fit two separate models, one testing the first hypothesis (contrast between semantically related and unrelated distractors) and the other testing the second hypothesis (contrast between semantically unrelated and uknown pseudoword distractors)

2. We could fit one model testing both hypotheses (and contrasts) simultaneously.

Whatever we choose, we need to code the specified contrasts first. Understanding of contrast coding is essential, and I will now show you why.

## Option 1: Treatment vs. sum contrasts for a factor with 2 levels

Imagine we opt for option 1 and restrict our dataset to data points with semantically related and unrelated distractors:

```{r}
dataOpt1 <- dataCorr2 %>%
  filter(Cond == "SemRelG" | Cond == "SemUnRelG")
```

Don't forget to adjust the levels of `Cond`:

```{r}
levels(dataOpt1$Cond) # Baseline is still included
dataOpt1$Cond <- factor(dataOpt1$Cond)
levels(dataOpt1$Cond) # Baseline is no longer there
```


Now imagine we do not know anything about contrast coding and we simply fit the model without giving it much thought (don't worry about not including random terms at the moment:

```{r}
model1 <- lm(RT ~ Cond, data = dataOpt1)

round(summary(model1)$coef,3)
```

How does R arrive at these particular values for the intercept and slope? We can find this out by inspecting the current contrasts of the factor `Cond` using the `contrasts()` command:

```{r}
contrasts(dataOpt1$Cond)
```

The default in R, often called **dummy coding**, is to code factors using treatment contrasts, whereby factor levels are ordered alphabetically. That's why level `SemRelG` is coded as 0 and level `SemUnRelG` is coded as 1. 

Now, what does that mean for our model?

$$
RT \sim \beta_0 + \beta_1*Cond 
$$ 

* Intercept $\beta_0$ is the estimated value (RT) for `SemRelG` 
* Slope $\beta_1$ is the estimated difference between the means of the two levels (i.e., `SemUnRelG`-`SemRelG`). The sign of the slope is negative because RTs are slower in `SemRelG`

$$
RT_{Related} = 847 - 0*27 = 847\\
RT_{Unrelated} = 847 - 1*27  = 820
$$ 

From a theoretical point of view, the intercept in the treatment assesses the average response in the baseline condition (in this case, in the semantically related condition), while the slope tests the difference between condition means. This also means that the intercept expresses a null hypothesis that is of no interest to us: that the mean in condition `SemRelG` is 0:

$$
H_0: \beta_0 = 0\\
H_0: \mu_{Unrelated} - \mu_{Related} = 0
$$

This contrast coding does not reflect the comparisons we would like to make and so does not correspond to the theoretical knowledge about how cognitive processes work. Therefore, it is better to use  **sum contast coding** instead:

```{r}
dataOpt1$Condition <- ifelse(dataOpt1$Cond == "SemRelG", -1, 1)

# we can also do:
contrasts(dataOpt1$Cond) <- c(-1,+1)
contrasts(dataOpt1$Cond)
```

Now, `SemRelG` is coded as -1 and `SemUnRelG` is coded as 1. What do the intercept and the slope represent now?

```{r}
model2 <- lm(RT ~ Cond, data = dataOpt1)

round(summary(model2)$coef,3)
```

* Intercept $\beta_0$ is now the estimated mean of the two conditions
* Slope $\beta_1$ is still the estimated difference between the means of the two conditions (i.e., `SemUnRelG`-`SemRelG`)

$$
RT_{GrandMean} = 834 - 0*14 = 834\\
RT_{Unrelated} = 834 - 1*14  = 820\\
RT_{Related} = 834 + 1*14  = 848\\
$$

Importantly, while the slope still assesses the difference in condition means, the intercept now tests the null hypothesis that the average of the two conditions is 0:

$$
H_0: \frac{\mu_{Unrelated} + \mu_{Related}}{2} = 0\\
H_0: \mu_{Unrelated} - \mu_{Related} = 0
$$

To summarise:

<style>
div.blue{background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

* Treatment contrasts compare one or more means against some (arbitrarily chosen by default in R) baseline condition.
* Sum contrasts compare a condition's mean against the grand mean. If we only have two conditions, this basically tests whether the two condition's means are identical.

</div>
<p>  </p>

## Option 2: Sum contrasts for a factor with 3 levels

Let us now think how we can test the two contrats (related vs. unrelated, and unrelated vs. unknown) in one model.

Basically, we want to code the factor `Cond` in such a way so that, for the first comparison, it contrasts the semantically related condition with the semantically unrelated condition while it disregards the unknown pseudoword condition, and, for the second comparison, it contrasts the unrelated condition with the unknown condition while ignoring the related condition.

Contrast 1:

$$
H_0: \mu_{Unrelated} - \mu_{Related} = 0
$$
Contrast 2:

$$
H_0: \mu_{Unrelated} - \mu_{Unknown} = 0
$$

And the null hypothesis for the intercept:

\begin{align}

H_0 &= \frac{\mu_{Unrelated} + \mu_{Related} + \mu_{Unknown}}{3} \\
&= \frac{1}{3} \mu_{Unrelated} + \frac{1}{3} \mu_{Related} + \frac{1}{3} \mu_{Unknown} \\
&= 0

\end{align}

We can summarise this as follows:

| Cond      | Contrast 1  | Contrast 2  | Intercept |
|-----------|-------------|-------------|-----------|
| SemRelG   | 1           | 0           | 1/3       |
| SemUnRelG | -1          | -1          | 1/3       |
| Baseline  | 0           | 0           | 1/3       |

and store this in a **hypothesis matrix**:

```{r}
(fractions(t(matrix <- rbind(int = 1/3, 
                       c1 = c(Baseline = 0, SemRelG = 1, SemUnRelG = -1),
                       c2 = c(Baseline = 1, SemRelG = 0, SemUnRelG = -1)))))
```

To obtain a contrast matrix necessary to test these hypotheses in a linear model, this matrix has to be inverted (see [Friendly, Fox & Chalmers, 2018](https://cran.r-project.org/web/packages/matlib/vignettes/ginv.html) and [Schad et al., 2019](https://arxiv.org/abs/1807.10451) for more detail):

```{r}
# function that formats the output of ginv():
ginv2 <- function(x)
 fractions(provideDimnames(ginv(x), base = dimnames(x)[2:1]))

# Invert matrix:
(matrix2 <- ginv2(matrix))
# Assign matrix to variable Cond:
(contrasts(dataCorr2$Cond) <- matrix2[, 2:3])
```

In the next step, we add two new columns to the original dataset to be able to convert from a factor-based random-effects structure to a vector-valued one:

```{r}
m0 <- lmer(RT ~ Cond + (1 + Cond||Subj) + (1 + Cond||TargetAnswer), data = dataCorr2)

mat <- model.matrix(m0)
dataCorr2$SemRel.SemUnRel <- mat[, 2]
dataCorr2$Bas.SemUnRel <- mat[, 3]

dataCorr2[1:5,18:ncol(dataCorr2)]
```

# Transform or not transform?

The core assumptions of the linear mixed effects models is that the residuals and random effect coefficients are independent and identically distributed. Ideally, the residuals should be normally distributed, and, if this assumption is violated, the model is simply inadequate.

```{r}
m1 <- lm(RT ~ Cond, data = dataCorr2)
hist(residuals(m1))
qqPlot(residuals(m1))
```

A quantile-quntile (QQ) plot is a scatterplot, in which two sets of quantiles are plotted against each other. If both sets of quantiles came from the same distribution, the points should form a straight line. By default, `qqplot()` produces a normal QQ plot and so allows us to check whether the model residuals are normally distributed (you can also use `qqnorm()`).

It would seem that the model residuals have more extreme values than would be expected if they were normally distributed.

How can we deal with that? There are 2 (main) options:

1. transfrom the data such that it satisfies the model assumptions
2. use generalised linear mixed effects models with a different (not normal) distribution

There is an active debate about which approach is best. 

The most important reasons for transforming the raw data have to do with some of the core properties of raw RTs (see e.g., [Schramm & Rouder, 2019](https://psyarxiv.com/9ksa6/) for more detail):

1. RTs aee unimodal with a skewed upper tail.
2. Manipulations that slow RTs tend to increase both the mean and SD with relatively small effects in the skewed tail. While such distributions as gamma, inverse Gaussian, ex-Gaussian, ex-Wald, lognormal, Weibull and Gumble can accomodate this feature, the normal distribution cannot.
3. Effects across conditions may be hard to detect because there is a lot of trial-by-trial variability as well as variability across participants.

Due to these properties, many statisticians believe that transformations may help reduce skewness (by minimising the impact of outliers) and stabilise variance by maintaining good power. This, in turn, means that transformations might make it easier to assess small effects. 

However, another property of RTs is that all RT distributions have a substantial shift away from zero. If such distributions are transformed, the shape and the scale of the resulting transformed distribution might be radically different from the orginal distribution of raw RTs. Furthermore, because the degree of variance is a function of the shift, transformations may work for some paradigms but not for others.

[Lo & Andrews, 2015](https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full) have argued that the best approach would be to analyse raw RTs with generalised linear mixed effects model. [Schramm & Rouder, 2019](https://psyarxiv.com/9ksa6/) used data simulations to show that lognormal transformations do not offer any advantages as compared to raw data, while reciprocal transformations might even decrease the statistical power (albeit by a small margin). Some researchers argue that the best practice would be to conduct both types of analyses (on raw and transformed data) and only consider the effects significant if both models produced a significant outcome(e.g., [Brysbaert & Stevens, 2018](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6646942/)). Importantly, however, the distributions can be seen as generating processes and thus have cognitive interpretations [(e.g., De Boeck & Jeon, 2019)](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00102/full), which means that choosing an appropriate distribution can be tricky.

It is beyond the scope of today's webinar to discuss this in depth so, in the following sections, I will give you examples on how to fit linear mixed effects models on both transformed and raw data. 

## How to transform?

```{r}
boxcox(m1)
```

The Box-Cox test suggests that we need an inverse transformation.

```{r}
# Add a new column transRT with inverse RTs:
dataCorr3 <- dataCorr2 %>%
  mutate(transRT = -10000/RT)
# Check:
plot(density(dataCorr3$transRT))
qqnorm(dataCorr3$transRT) 
# Fit a model on inverse RTs and check residuals:
mInv <- lm(transRT ~ Cond, data = dataCorr3)
hist(residuals(mInv))
qqPlot(residuals(mInv))
```

# Summary statistics and plots

Let's visualise the data before we fit the models.

```{r}
(RT <- summarySEwithin(dataCorr3, measurevar = "RT", withinvars = "Cond",
                       idvar = "Subj", na.rm = FALSE, conf.interval = .95))
```

## Plot all conditions

```{r}
ggplot(RT, aes(x = Cond, y = RT, fill = Cond)) +
  geom_bar(position = position_dodge(), stat = "identity",
           color = "black",
           size = .3) +
  geom_errorbar(aes(ymin = RT-se, ymax = RT+se),
                size = .3, width = .2,
                position = position_dodge(.9)) +
  xlab("Distractor condition") +
  ylab("Response time (ms)") +
  coord_cartesian(ylim = c(500,900)) +
  theme_classic() +
  theme(axis.title.x = element_text(size = rel(1.2), colour = "black"),
        axis.title.y = element_text(size = rel(1.2), colour = "black"),
        panel.background = element_rect(colour = "white"),
        axis.text = element_text(size = rel(1), colour = "black"),
        legend.text = element_text(size = rel(1), colour = "black"),
        legend.title = element_text(size = rel(1.2), colour = "black"),
        axis.line = element_line(colour = "black")) +
     scale_fill_manual(name = "Distractor condition", 
                     labels = c("Unknown pseudoword", "German sem. related", "German sem. unrelated"), 
                     values = c("#D95F02", "#440154FF", "#35B779FF")) + guides(fill = FALSE) +
  scale_x_discrete(labels = c("Unknown pseudoword", "German sem. related", "German sem. unrelated"))
```

## Plot contrast 1 (related vs. unrelated)

```{r}
dataG <- dataCorr3 %>%
  filter(Cond == "SemRelG" | Cond == "SemUnRelG")
dataG$Cond <- factor(dataG$Cond)

RT1 <- summarySEwithin(dataG, measurevar = "RT", withinvars = "Cond",
                       idvar = "Subj", na.rm = FALSE, conf.interval = .95)

ggplot(RT1, aes(x = Cond, y = RT, fill = Cond)) +
  geom_bar(position = position_dodge(), stat = "identity",
           color = "black", 
           size = .3) +
  geom_errorbar(aes(ymin = RT-se, ymax = RT+se),
                size = .3, width = .2,
                position = position_dodge(.9)) +
  xlab("Distractor condition") +
  ylab("Response time (ms)") +
  coord_cartesian(ylim = c(500,950)) +
  theme_classic() +
  theme(axis.title.x = element_text(size = rel(1.2), colour = "black"),
        axis.title.y = element_text(size = rel(1.2), colour = "black"),
        panel.background = element_rect(colour = "white"),
        axis.text = element_text(size = rel(1), colour = "black"),
        legend.text = element_text(size = rel(1), colour = "black"),
        legend.title = element_text(size = rel(1.2), colour = "black"),
        axis.line = element_line(colour = "black")) +
      scale_fill_manual(name = "Distractor condition", 
                     labels = c("German sem. related", "German sem. unrelated"), 
                     values = c("#440154FF", "#35B779FF")) + guides(fill = FALSE) +
  scale_x_discrete(labels = c("German sem. related", "German sem. unrelated"))
```

## Plot contrast 2 (unknown vs. unrelated)

```{r}
dataGU <- dataCorr3 %>%
  filter(Cond == "SemUnRelG" | Cond == "Baseline")

RT2 <- summarySEwithin(dataGU, measurevar = "RT", withinvars = "Cond",
                       idvar = "Subj", na.rm = FALSE, conf.interval = .95)

ggplot(RT2, aes(x = Cond, y = RT, fill = Cond)) +
  geom_bar(position = position_dodge(), stat = "identity",
           color = "black",
           size = .3) +
  geom_errorbar(aes(ymin = RT-se, ymax = RT+se),
                size = .3, width = .2,
                position = position_dodge(.9)) +
  xlab("Distractor condition") +
  ylab("Response time (ms)") +
  coord_cartesian(ylim = c(500,900)) +
  theme_classic() +
  theme(axis.title.x = element_text(size = rel(1.2), colour = "black"),
        axis.title.y = element_text(size = rel(1.2), colour = "black"),
        panel.background = element_rect(colour = "white"),
        axis.text = element_text(size = rel(1), colour = "black"),
        legend.text = element_text(size = rel(1), colour = "black"),
        legend.title = element_text(size = rel(1.2), colour = "black"),
        axis.line = element_line(colour = "black")) +
     scale_fill_manual(name = "Distractor condition", 
                     labels = c("Unknown pseudoword", "German sem. unrelated"), 
                     values = c("#D95F02", "#35B779FF")) + guides(fill = FALSE) +
  scale_x_discrete(labels = c("Unknown pseudoword","German sem. unrelated"))
```

# Fit the model

There are two main approaches to model building:

1. Start with a *minimal* model that captures the phenomenon of interest but not much other structure in the data (e.g., a linear model with just the factor of main interest). Next, perform a number of checks (such as, but not limited to, if the model assumptions are met and whether the model fit is good) and, if the model passes all of them, additional structures can be added. If the model turns out to be inadequate, improve the model or start a new cycle of model development.

$$
RT \sim \beta_0 + \beta_1*Cond
$$

2. Start with a *maximal* model, i.e., a model that contains all effects from the experimental manipulations (main effects and interactions) and all within-subject and within-item variance components. Note that the maximal model is maximal within the scope of a linear regression; however, it is not maximal with respect to the data-generating process (i.e., it doesn't capture things like selection bias, changes across time, etc.).  

<style>
div.blue{background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

What is a maximal model?

* *Intercept*
* *Fixed effects*
* *Random effects for subjects:*
  + the by-subjects adjustment to the grand mean 
  + the by-subjects adjustment to the mean slope
 
* *Random effects for items:*
  + the by-subjects adjustment to the grand mean 
  + the by-subjects adjustment to the mean slope
  
* *Correlations between the adjustments for the intercepts* 
* *Correlations between the adjustments for the slopes*  

* *Residual error*

Mathematically, it looks like this:

$$
RT \sim \beta_0 + u_0 + w_0 + (\beta_1 + u_1 + w_1)*Cond + \varepsilon
$$

where 

* $\beta_0$ is the intercept parameter
* $\beta_1$ is the the slope parameter
* $\varepsilon$ is the residual
* `Cond` is the effect of Condition

and the *variance components* are
 
* $u_0$: adjustment to the intercept by subjects
* $w_0$: adjustment to the intercept by items
* $u_1$: adjustment to the slope by subjects
* $w_1$: adjustment to the slope by items
  
with the *correlations between the adjustments for the intercepts and slopes* expressed in the following matrices

\[
\left( 
\begin{array}{cc}
u_0 \\ 
u_1 
\end{array} 
\right) \sim \left(Normal _2\left(
\begin{array} {cc}
0 \\
0
\end{array}
\right), \left[
\begin{array}{cc}
\sigma u_0^2 & \rho \sigma u_0\sigma u_1 \\ 
\rho \sigma u_0\sigma u_1 & \sigma u_1^2
\end{array}
\right]
\right)
\]

\[
\left( 
\begin{array}{cc}
w_0 \\ 
w_1 
\end{array} 
\right) \sim \left(Normal _2\left(
\begin{array} {cc}
0 \\
0
\end{array}
\right), \left[
\begin{array}{cc}
\sigma w_0^2 & \rho \sigma w_0\sigma w_1 \\ 
\rho \sigma w_0\sigma w_1 & \sigma w_1^2
\end{array}
\right]
\right)
\]

where

$\rho$ is the correlation parameter.

</div>
<p>  </p>
  
Imagine you choose option 2. What do you do if the model fails? 

In fact, after [Barr et al., 2013](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/) had recommended to only fit models with the maximal random effects structure, many researchers have been having trouble getting the models to converge. The main problem with maximal models though is not convergence errors but overparametrisation, a situation, when the model is so complex that it is unable to provide accurate estimates (e.g., because there is simply not enough data) and is uninterpretable.

An alternative approach is, therefore, to fit a model that is **supported by the data** [(Bates et al., 2015)](https://arxiv.org/abs/1506.04967). This means developing a model, in which all variance components and correlation parameters are supported by the data. 

## Option 1 (transformed RTs)

In this webinar, we will start by fitting a model with a maximal structure and then simplify it by assessing the random effects structure with the Principled Component Analysis (PCA). 

```{r}
maxmod <- lmer(transRT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + Bas.SemUnRel + SemRel.SemUnRel|Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel|TargetAnswer), dataCorr3)
```

Check the structure of the model:

```{r}
summary(rePCA(maxmod))
VarCorr(maxmod)
```

There is one component - the adjustment to the slope for `Bas.SemUnRel` - that does not explain any variance; it should be removed.

Note that, at this stage, we do not even need to check whether the model converged or not. And we certainly do not need to check the p-values.

```{r}
mod2 <- lmer(transRT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + SemRel.SemUnRel|Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel|TargetAnswer), dataCorr3)
```

Run PCA:

```{r}
summary(rePCA(mod2))
VarCorr(mod2)
```

Looks better. Let's make sure that the model provides a good fit to the data.

We start by checking whether the residuals follow the normal distribution:

```{r}
qqnorm(resid(mod2))
plot(fitted(mod2), resid(mod2)) 
```

They do, and there isn't any evidence of heteroscedasticity in the residuals against fitted values plot.

Now we can finally take a look at the results.

```{r}
print(summary(mod2, corr = F))
```

Two things to note before we talk about the results:

1. This model was fit using **REML (Restricted Maximum Likelihood Estimation)**, which is a way to estimate variance components. REML works by first getting regression residuals for the observations modeled by the fixed effects portion of the model. At this point, it ignores any variance components and estimates the statistical model for these residuals. Next, it uses **MLE (maximum likelihood estimation)** on the residuals to get estimates of the variance components. The main advantage of this approach is that the MLE adjusts the variance estimates for the fact that we are working with regression residuals ([here](http://users.stat.umn.edu/~gary/classes/5303/handouts/REML.pdf) is a super short paper/handout on this for those who want to know more). It's generally good to use REML, if it is available, when you are interested in the magnitude of the random effects variances, but never when you are comparing models with different fixed effects via hypothesis tests or information-theoretic criteria such as AIC.

2. Optimisers: If you have tons of data, modelling with `lme4` can take time. This is partly due to `lme4` performing thousands of convergence checks. The choice of an optimiser can change the results of the model so it is recommended to fit models with different optimisers and then compare the outcomes. [Here](http://svmiller.com/blog/2018/06/mixed-effects-models-optimizer-checks/) is a useful and short blog post about it. Importantly, make sure you understand how optimsers work and how they differ from each other before you use them.

Alright... It would seem that we cannot reject the null for the second contrast (semantically unrelated vs. unknown) but that we can for the first contrast (semantically related vs. unrelated).

**Confidence intervals**

```{r}
# this will take some time!!
ConfidInt <- confint(mod2, parm = c("(Intercept)", "Bas.SemUnRel", "SemRel.SemUnRel"), method = "boot")
round(ConfidInt, 3)
```

**Table of effects**

```{r}
labels <- c("Contrast 1: German sem. unrelated vs. untrained pseudoword distractors", "Contrast 2: German sem. related vs. unrelated distractors")

keep.terms <- names(fixef(mod2)[-1])

tab_model(mod2, 
          terms = keep.terms, 
          auto.label = FALSE, 
          pred.labels = labels, 
          show.se = TRUE, show.stat = TRUE, 
          show.ci = FALSE, string.se = "SE", 
          show.re.var = FALSE, 
          show.obs = FALSE, show.ngroups = FALSE,
          emph.p = FALSE, dv.labels = "Dependent Variable", show.icc = FALSE)
```

Note that marginal $R^2$ shows the variance explained only by the fixed effects, while conditional $R^2$ provides the variance explained by the entire model (including the random effects).

**Plot of effects**

```{r}
plot_model(mod2, title = "", terms = keep.terms, axis.labels = rev(labels), 
           type = "est", sort.est = NULL, colors = "bw", 
           show.values = TRUE, show.p = TRUE, value.offset = 0.4, 
           value.size = 4, dot.size = 2, line.size = 1, 
           vline.color = "black", 
           width = 0.1) + theme_sjplot2() +
           scale_color_sjplot(palette = "circus")
```

## Option 2 (raw RTs)

We will fit a generalised linear mixed effects model with a gamma distribution and identitylink.

The choice of the gamma distribution is mainly motivated by considerations outlined in [Lo & Andrews (2015)](https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full). To summarise briefly:

* The Gamma distribution is the sum of multiple exponential distributions. Basically, it models the probability that no event occurs until a certain period of time. Therefore, it could be conceptualised as modelling several serial stages of processing, each of which finishes with a time that is exponentially distributed (see e.g., [Van Zandt & Ratcliff, 1995](https://link.springer.com/article/10.3758/BF03214411)).

* In GLM and GLME models, fixed factors are treated as linear predictors of a function of the observed response rather than the observed response itself. Because we assume that RT is a direct measure of the time required to name a picture, the function binding the expected values of the DV and the effect of the predictor is the identity link.

```{r}
maxmod_raw <- glmer(RT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + Bas.SemUnRel + SemRel.SemUnRel|Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel|TargetAnswer), dataCorr3, family = Gamma(link = "identity"))

summary(rePCA(maxmod_raw))
VarCorr(maxmod_raw)
```

Simplify:

```{r}
mod2_raw <- glmer(RT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + Bas.SemUnRel + SemRel.SemUnRel|Subj) + 
                (1 + SemRel.SemUnRel|TargetAnswer), dataCorr3, family = Gamma(link = "identity"))

summary(rePCA(mod2_raw))
VarCorr(mod2_raw)
```