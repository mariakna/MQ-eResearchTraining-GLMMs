---
title: "Data analysis with (generalised) linear mixed effects models in R"
author: "Maria Korochkina"
date: "Last update: `r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 5
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    theme: cerulean
fontsize: 16pt
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE)
```

# Load required packages

```{r, results = "hide"}
rm(list=ls())

library("openxlsx")
library("Rmisc")
library("tidyverse")
library("lme4")
library("car")
library("MASS")
library("scales")
library("lmerTest")
library("sjmisc")
library("sjPlot")
library("ggsignif")
library("RColorBrewer")
library("rms")
library("broom.mixed")
```

# Response times

## Load data

Data that we will be using today is available [here](https://github.com/mariakna/MQ-eResearchTraning--G-LMEmodels). 

Download the file *pwiExp.xlsx*, save it in a directory of your choice, make this directory your working directory by using the command `setwd()` and load the file as shown below:

```{r}
data <- read.xlsx("pwiExp.xlsx", sheet = 1)
```

### Data description

This file contains data from a picture-word interference task. In this task, German native speakers named (in German) pictures of familiar concepts (e.g., elephant) with superimposed written word distractors. The distractors were either semantically related (e.g., "cat") or unrelated (e.g., "hammer") to the depicted concepts. The distractors were (a) German words, (b) novel words that the participants had learned earlier in the experiment, or (c) unknown pseudowords. 

<center>
![FigName](pwi.png){width=300px}
</center>

The data we just loaded contains lots of information, but the relevant variables for us today are:

* `Subj`: Participant ID (N = 60)
* `Cond`: Condition, a factor with 5 levels: `SemRelG` (semantically related German), `SemUnRelG` (semantically unrelated German), `SemRelN` (semantically related novel), `SemUnRelN` (semantically unrelated novel), `Baseline` (unknown pseudowords as distractors)
* `TargetAnswer`: correct picture name (in German)
* `Response`: participant's response
* `Accuracy`: a factor with 2 levels, `0` = incorrect responses, `1` = correct responses
* `RT`: response times

<style>
div.blue{background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

**Hypotheses 1 & 2:**

1. Compared to unknown pseudowords, German semantically unrelated distractors will slow picture naming response times.
2. Compared to semantically unrelated German distractors, German distractors from the same semantic category will slow picture naming response times.

</div>
<p>  </p>

Restrict the dataset to German and unknown pseudoword distractors:

```{r}
data2 <- data %>%
  filter(Cond == "SemRelG" | Cond == "SemUnRelG" | Cond == "Baseline")
```

Adjust factor levels for the independent variable and random terms:

```{r}
data2$Cond <- factor(data2$Cond)
levels(data2$Cond)

data2$Subj <- factor(data2$Subj)
data2$TargetAnswer <- factor(data2$TargetAnswer)
```

## Inspect data

This step is essential and should not be skipped as it allows the researcher to ensure that the data looks as it should. You can check your data in any way you want. I often use a function called `xtabs()`:

```{r}
head(xtabs(~ Subj + List,data2)) 
head(xtabs(~ Subj + Cond,data2)) 
xtabs(~ ItemType + Cond,data2) 
xtabs(~ List + Cond,data2) 
```

## Prepare data for analysis

### Remove incorrect responses

Check accuracy and exclude observations with incorrect responses.

Overall accuracy:

```{r}
ftable(data2$Accuracy)
```

Accuracy per condition:

```{r}
(Accuracy <- summarySEwithin(data2, measurevar = "Accuracy", withinvars = "Cond",
                       idvar = "Subj", na.rm = FALSE, conf.interval = .95))
```

**Note:** The function `summarySEwithin` is a good (although not yet widely accepted) method to summarise data for experiments with a within-subjects design. The standard SE formula

$$
\sigma_{\overline{x}} = \frac{s}{\sqrt{n}}
$$

does  not take into  account  that there are multiple  data  points  from  each  participant  and, instead,  summarises  the  variability  across  all  observations  as  if  they  were  independent. Similarly, aggregating the data and calculating averages per participant per condition would conflate variability associated with each participantâ€™s performance and random error with the variability associated with the experimental manipulation. One method  to  compute  standard error while also disentangling these two sources of variability was described in [Morey (2008)](http://pcl.missouri.edu/sites/default/files/morey.2008.pdf) and later implemented in the [Rmisc package](http://www.cookbook-r.com/Graphs/Plotting_means_and_error_bars_(ggplot2)/) by Ryan Hope. 

Luckily, the participants are at ceiling! We can now restrict the data to correct responses:

```{r}
dataCorr <- data2 %>%
  filter(Accuracy == 1)
```

### Data trimming

There are many ways to do it, and it often depends on your research question and your knowledge about the processes you are studying. For example, in word production, it often makes sense to remove data points with super fast and super slow responses. 

I quite like the approach of keeping as many observations as you can and trying to model them so I often start with inspecting the data distribution and only removing those data points that clearly stand out:

```{r}
plot(density(dataCorr$RT))
```

Based on the data distribution and what we know about picture naming, we remove responses faster than 300ms and slower than 2000ms. 

```{r}
dataCorr2 <- dataCorr %>%
 filter(RT > 300 & RT < 2000) # 12 obs removed
```

## Contrast coding

<style>
div.blue{background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

**Hypotheses 1 & 2:**

1. Compared to unknown pseudowords, German semantically unrelated distractors will slow picture naming response times.
2. Compared to semantically unrelated German distractors, German distractors from the same semantic category will slow picture naming response times.

</div>
<p>  </p>

How can we test these hypotheses?

1. We could fit two separate models, one testing the first hypothesis (contrast between the semantically unrelated and uknown pseudoword distractors) and the other testing the second hypothesis (contrast between the semantically related and unrelated distractors).

2. We could fit one model testing both hypotheses (and contrasts) simultaneously.

Whatever we choose, we need to code the specified contrasts first. Good understanding of contrast coding is essential, and I will now show you why.

### Option 1: Treatment vs. sum contrasts for a factor with 2 levels

Imagine we opt for option 1 and restrict our dataset to data points with the semantically related and unrelated distractors to address hypothesis 2:

```{r}
dataOpt1 <- dataCorr2 %>%
  filter(Cond == "SemRelG" | Cond == "SemUnRelG")
```

Don't forget to adjust the levels of `Cond`:

```{r}
levels(dataOpt1$Cond) # Baseline is still included
dataOpt1$Cond <- factor(dataOpt1$Cond)
levels(dataOpt1$Cond) # Baseline is no longer there
```

Now imagine that we do not know anything about contrast coding and simply fit the model without giving it much thought (don't worry about not including random terms at the moment):

```{r}
model1 <- lm(RT ~ Cond, data = dataOpt1)

round(summary(model1)$coef,3)
```

How does R arrive at these particular values for the intercept and the slope? We can find out by inspecting the current contrasts of the factor `Cond`:

```{r}
contrasts(dataOpt1$Cond)
```

What you can see is the default option in R, often called **dummy coding**. Here, the are coded factors using treatment contrasts, whereby factor levels are ordered alphabetically. That's why level `SemRelG` is coded as 0 and level `SemUnRelG` is coded as 1. 

Now, what does that mean for our model?

$$
RT \sim \beta_0 + \beta_1*Cond 
$$ 

* Intercept $\beta_0$ is the estimated value (RT) for `SemRelG`. 
* Slope $\beta_1$ is the estimated difference between the means of the two levels (i.e., `SemUnRelG`-`SemRelG`). The sign of the slope is negative because RTs are slower in `SemRelG`.

$$
RT_{Related} = 847 - 0*27 = 847\\
RT_{Unrelated} = 847 - 1*27  = 820
$$ 

From a theoretical point of view, the intercept in the treatment assesses the average response in the baseline condition (in this case, in the semantically related condition), while the slope tests the difference between condition means. This also means that the intercept expresses a null hypothesis that is of no interest to us - that the mean RT in condition `SemRelG` is 0:

$$
H_0: \beta_0 = 0
$$

and

$$
H_0: \mu_{Unrelated} - \mu_{Related} = 0
$$

Obviously, this contrast coding does not reflect the comparisons we would like to make and also does not correspond to the theoretical knowledge about how cognitive processes work. We are better off with  **sum contasts** instead:

```{r}
contrasts(dataOpt1$Cond) <- c(-1,+1)
contrasts(dataOpt1$Cond)

# you can also create a separate variable that contains values -1 and 1:
dataOpt1$Condition <- ifelse(dataOpt1$Cond == "SemRelG", -1, 1)
```

Now, `SemRelG` is coded as -1 and `SemUnRelG` is coded as 1. What do the intercept and the slope represent now?

```{r}
model2 <- lm(RT ~ Cond, data = dataOpt1)

round(summary(model2)$coef,3)
```

* Intercept $\beta_0$ is now the estimated mean of the two conditions.
* Slope $\beta_1$ is still the estimated difference between the means of the two conditions (i.e., `SemUnRelG`-`SemRelG`).

$$
RT_{GrandMean} = 834 - 0*14 = 834\\
RT_{Unrelated} = 834 - 1*14  = 820\\
RT_{Related} = 834 + 1*14  = 848\\
$$

Importantly, while the slope still assesses the difference in condition means, the intercept now tests the null hypothesis that the average of the two conditions is 0:

$$
H_0: \frac{\mu_{Unrelated} + \mu_{Related}}{2} = 0
$$

and

$$
H_0: \mu_{Unrelated} - \mu_{Related} = 0
$$

To summarise:

<style>
div.blue{background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

* Treatment contrasts compare one or more means against some (arbitrarily chosen by default in R) baseline condition.
* Sum contrasts compare a condition's mean against the grand mean. If we only have two conditions, this basically tests whether the two condition's means are identical.

</div>
<p>  </p>

### Option 2: Sum contrasts for a factor with 3 levels

Let us now think how we can test the two contrats (unrelated vs. unknown, and related vs. unrelated) in one model.

Contrast 1:

$$
H_0: \mu_{Unrelated} - \mu_{Related} = 0
$$

Contrast 2:

$$
H_0: \mu_{Unrelated} - \mu_{Unknown} = 0
$$

And the null hypothesis for the intercept:

\begin{align}

H_0 &= \frac{\mu_{Unrelated} + \mu_{Related} + \mu_{Unknown}}{3} \\
&= \frac{1}{3} \mu_{Unrelated} + \frac{1}{3} \mu_{Related} + \frac{1}{3} \mu_{Unknown} \\
&= 0

\end{align}

We can summarise this as follows:

| Cond      | Contrast 1  | Contrast 2  | Intercept |
|-----------|-------------|-------------|-----------|
| SemRelG   | 0           | 1           | 1/3       |
| SemUnRelG | -1          | -1          | 1/3       |
| Baseline  | 1           | 0           | 1/3       |

and store in a **hypothesis matrix**:

```{r}
(fractions(t(matrix <- rbind(int = 1/3, 
                       c1 = c(Baseline = 1, SemRelG = 0, SemUnRelG = -1),
                       c2 = c(Baseline = 0, SemRelG = 1, SemUnRelG = -1)))))
```

To obtain a contrast matrix necessary to test these hypotheses in a linear model, the hypothesis matrix has to be inverted (see [Friendly, Fox & Chalmers, 2018](https://cran.r-project.org/web/packages/matlib/vignettes/ginv.html) and [Schad et al., 2019](https://arxiv.org/abs/1807.10451) for more detail):

```{r}
# function that formats the output of ginv():
ginv2 <- function(x)
 fractions(provideDimnames(ginv(x), base = dimnames(x)[2:1]))

# Invert matrix:
(matrix2 <- ginv2(matrix))
# Assign matrix to the variable Cond:
(contrasts(dataCorr2$Cond) <- matrix2[, 2:3])
```

In the next step, we add two new columns to the original dataset to be able to convert from a factor-based random-effects structure to a vector-valued one:

```{r}
m0 <- lmer(RT ~ Cond + (1 + Cond||Subj) + (1 + Cond||TargetAnswer), data = dataCorr2)

mat <- model.matrix(m0)
dataCorr2$Bas.SemUnRel <- mat[, 2]
dataCorr2$SemRel.SemUnRel <- mat[, 3]

dataCorr2[1:5,18:ncol(dataCorr2)]
```

## Transform or not transform?

The core assumption of the linear mixed effects models is that the residuals and random effect coefficients are independent and identically distributed. Ideally, the residuals should be normally distributed, and, if this assumption is violated, the model is simply inadequate.

```{r}
m1 <- lm(RT ~ Cond, data = dataCorr2)
hist(residuals(m1))
qqPlot(residuals(m1))
```

A **quantile-quntile (QQ) plot** is a scatterplot, in which two sets of quantiles are plotted against each other. If both sets of quantiles came from the same distribution, the points should form a straight line. By default, `qqplot()` produces a normal QQ plot and so allows us to check whether the model residuals are normally distributed (you can also use `qqnorm()`).

It would seem that the model residuals have more extreme values than would be expected if they were normally distributed.

How can we deal with that? There are 2 (main) options:

1. Transfrom the data such that it satisfies the model assumptions.
2. Use the generalised linear mixed effects model with a different (not normal) distribution.

There is an active debate about which approach is best. 

The most important reasons for transforming the raw data have to do with some of the core properties of raw RTs (see e.g., [Schramm & Rouder, 2019](https://psyarxiv.com/9ksa6/) for more detail):

1. RTs are unimodal with a skewed upper tail.
2. Manipulations that slow RTs tend to increase both the mean and SD with relatively small effects in the skewed tail. While such distributions as gamma, inverse Gaussian, ex-Gaussian, ex-Wald, lognormal, Weibull and Gumble can accomodate this feature, the normal distribution cannot.
3. Effects across conditions may be hard to detect because there is a lot of trial-by-trial variability as well as variability across participants.

Due to these properties, many statisticians believe that transformations may help reduce skewness (by minimising the impact of outliers) and stabilise variance by maintaining good power. This, in turn, means that transformations might make it easier to assess small effects. 

However, another property of RTs is that all RT distributions have a substantial shift away from zero. If such distributions are transformed, the shape and the scale of the resulting transformed distribution might be radically different from the orginal distribution of raw RTs. Furthermore, because the degree of variance is a function of the shift, transformations may work for some paradigms but not for others.

[Lo & Andrews, 2015](https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full) have argued that the best approach would be to analyse raw RTs with the generalised linear mixed effects model. [Schramm & Rouder, 2019](https://psyarxiv.com/9ksa6/) used data simulations to show that lognormal transformations do not offer any advantages as compared to raw data, while reciprocal transformations might even decrease the statistical power (albeit by a small margin). Other researchers argue that the best practice would be to conduct both types of analyses (on raw and transformed data) and only consider the effects significant if both models produced a significant outcome (e.g., [Brysbaert & Stevens, 2018](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6646942/)). Importantly, because the distributions can be interpreted as data generating processes, the choice of a distribution often has cognitive interpretations [(e.g., De Boeck & Jeon, 2019)](https://www.frontiersin.org/articles/10.3389/fpsyg.2019.00102/full), and should be done very carefully.

It is beyond the scope of today's webinar to discuss this in depth so, in the following sections, I will give you examples on how to fit models on both transformed and raw data. 

### How to transform?

```{r}
boxcox(m1)
```

The Box-Cox test suggests that we need an inverse transformation.

```{r}
# Add a new column transRT with inverse RTs:
dataCorr3 <- dataCorr2 %>%
  mutate(transRT = -10000/RT)
# Check:
plot(density(dataCorr3$transRT))
qqnorm(dataCorr3$transRT) 
# Fit a model on inverse RTs and check residuals:
mInv <- lm(transRT ~ Cond, data = dataCorr3)
hist(residuals(mInv))
qqPlot(residuals(mInv))
```

## Summary statistics and plots

Let's visualise the data before we fit the models.

```{r}
(RT <- summarySEwithin(dataCorr3, measurevar = "RT", withinvars = "Cond",
                       idvar = "Subj", na.rm = FALSE, conf.interval = .95))
```

### Plot all conditions

```{r}
ggplot(RT, aes(x = Cond, y = RT, fill = Cond)) +
  geom_bar(position = position_dodge(), stat = "identity",
           color = "black",
           size = .3) +
  geom_errorbar(aes(ymin = RT-se, ymax = RT+se),
                size = .3, width = .2,
                position = position_dodge(.9)) +
  xlab("Distractor condition") +
  ylab("Response time (ms)") +
  coord_cartesian(ylim = c(500,900)) +
  theme_classic() +
  theme(axis.title.x = element_text(size = rel(1.2), colour = "black"),
        axis.title.y = element_text(size = rel(1.2), colour = "black"),
        panel.background = element_rect(colour = "white"),
        axis.text = element_text(size = rel(1), colour = "black"),
        legend.text = element_text(size = rel(1), colour = "black"),
        legend.title = element_text(size = rel(1.2), colour = "black"),
        axis.line = element_line(colour = "black")) +
     scale_fill_manual(name = "Distractor condition", 
                     labels = c("Unknown pseudoword", "German sem. related", "German sem. unrelated"), 
                     values = c("#D95F02", "#440154FF", "#35B779FF")) + guides(fill = FALSE) +
  scale_x_discrete(labels = c("Unknown pseudoword", "German sem. related", "German sem. unrelated"))
```

### Plot contrast 1 (related vs. unrelated)

```{r}
dataG <- dataCorr3 %>%
  filter(Cond == "SemRelG" | Cond == "SemUnRelG")
dataG$Cond <- factor(dataG$Cond)

RT1 <- summarySEwithin(dataG, measurevar = "RT", withinvars = "Cond",
                       idvar = "Subj", na.rm = FALSE, conf.interval = .95)

ggplot(RT1, aes(x = Cond, y = RT, fill = Cond)) +
  geom_bar(position = position_dodge(), stat = "identity",
           color = "black", 
           size = .3) +
  geom_errorbar(aes(ymin = RT-se, ymax = RT+se),
                size = .3, width = .2,
                position = position_dodge(.9)) +
  xlab("Distractor condition") +
  ylab("Response time (ms)") +
  coord_cartesian(ylim = c(500,950)) +
  theme_classic() +
  theme(axis.title.x = element_text(size = rel(1.2), colour = "black"),
        axis.title.y = element_text(size = rel(1.2), colour = "black"),
        panel.background = element_rect(colour = "white"),
        axis.text = element_text(size = rel(1), colour = "black"),
        legend.text = element_text(size = rel(1), colour = "black"),
        legend.title = element_text(size = rel(1.2), colour = "black"),
        axis.line = element_line(colour = "black")) +
      scale_fill_manual(name = "Distractor condition", 
                     labels = c("German sem. related", "German sem. unrelated"), 
                     values = c("#440154FF", "#35B779FF")) + guides(fill = FALSE) +
  scale_x_discrete(labels = c("German sem. related", "German sem. unrelated"))
```

### Plot contrast 2 (unknown vs. unrelated)

```{r}
dataGU <- dataCorr3 %>%
  filter(Cond == "SemUnRelG" | Cond == "Baseline")

RT2 <- summarySEwithin(dataGU, measurevar = "RT", withinvars = "Cond",
                       idvar = "Subj", na.rm = FALSE, conf.interval = .95)

ggplot(RT2, aes(x = Cond, y = RT, fill = Cond)) +
  geom_bar(position = position_dodge(), stat = "identity",
           color = "black",
           size = .3) +
  geom_errorbar(aes(ymin = RT-se, ymax = RT+se),
                size = .3, width = .2,
                position = position_dodge(.9)) +
  xlab("Distractor condition") +
  ylab("Response time (ms)") +
  coord_cartesian(ylim = c(500,900)) +
  theme_classic() +
  theme(axis.title.x = element_text(size = rel(1.2), colour = "black"),
        axis.title.y = element_text(size = rel(1.2), colour = "black"),
        panel.background = element_rect(colour = "white"),
        axis.text = element_text(size = rel(1), colour = "black"),
        legend.text = element_text(size = rel(1), colour = "black"),
        legend.title = element_text(size = rel(1.2), colour = "black"),
        axis.line = element_line(colour = "black")) +
     scale_fill_manual(name = "Distractor condition", 
                     labels = c("Unknown pseudoword", "German sem. unrelated"), 
                     values = c("#D95F02", "#35B779FF")) + guides(fill = FALSE) +
  scale_x_discrete(labels = c("Unknown pseudoword","German sem. unrelated"))
```

## Fit the model

There are two main approaches to model building (e.g., [Schad et al., 2019](https://arxiv.org/abs/1904.12765)):

1. Start with a *minimal* model that captures the phenomenon of interest but not much other structure in the data (e.g., a linear model with just the factor of main interest). Next, perform a number of checks (such as, but not limited to, if the model assumptions are met and whether the model fit is good) and, if the model passes all of them, additional structures can be added. If the model turns out to be inadequate, improve the model or start a new cycle of model development.

$$
RT \sim \beta_0 + \beta_1*Cond
$$

2. Start with a *maximal* model, i.e., a model that contains all effects from the experimental manipulations (main effects and interactions) and all within-subject and within-item variance components. Note that the maximal model is maximal within the scope of a linear regression; however, it is not maximal with respect to the data generating process (i.e., it doesn't capture things like selection bias, changes across time, etc.).  

<style>
div.blue{background-color:#e6f0ff; border-radius: 5px; padding: 20px;}
</style>
<div class = "blue">

What is a maximal model?

* *Intercept*
* *Fixed effects*
* *Random effects for subjects:*

  + the by-subjects adjustment to the grand mean 
  + the by-subjects adjustment to the mean slope
 
* *Random effects for items:*

  + the by-subjects adjustment to the grand mean 
  + the by-subjects adjustment to the mean slope
  
* *Correlations between the adjustments for the intercepts* 
* *Correlations between the adjustments for the slopes*  

* *Residual error*

Mathematically, it looks like this:

$$
RT \sim \beta_0 + u_0 + w_0 + (\beta_1 + u_1 + w_1)*Cond + \varepsilon
$$

where 

* $\beta_0$ is the intercept parameter
* $\beta_1$ is the the slope parameter
* $\varepsilon$ is the residual
* `Cond` is the effect of Condition

and the *variance components* are
 
* $u_0$: adjustment to the intercept by subjects
* $w_0$: adjustment to the intercept by items
* $u_1$: adjustment to the slope by subjects
* $w_1$: adjustment to the slope by items
  
with the *correlations between the adjustments for the intercepts and slopes* expressed in the following matrices

\[
\left( 
\begin{array}{cc}
u_0 \\ 
u_1 
\end{array} 
\right) \sim \left(Normal _2\left(
\begin{array} {cc}
0 \\
0
\end{array}
\right), \left[
\begin{array}{cc}
\sigma u_0^2 & \rho \sigma u_0\sigma u_1 \\ 
\rho \sigma u_0\sigma u_1 & \sigma u_1^2
\end{array}
\right]
\right)
\]

\[
\left( 
\begin{array}{cc}
w_0 \\ 
w_1 
\end{array} 
\right) \sim \left(Normal _2\left(
\begin{array} {cc}
0 \\
0
\end{array}
\right), \left[
\begin{array}{cc}
\sigma w_0^2 & \rho \sigma w_0\sigma w_1 \\ 
\rho \sigma w_0\sigma w_1 & \sigma w_1^2
\end{array}
\right]
\right)
\]

where

$\rho$ is the correlation parameter.

</div>
<p>  </p>
  
Imagine you choose option 2. What do you do if the model fails? 

In fact, after [Barr et al., 2013](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3881361/) had recommended to only fit models with the maximal random effects structure, many researchers have been having trouble getting their models to converge. The main problem with maximal models though is overparametrisation, a situation, when the model is so complex that it is unable to provide accurate estimates (e.g., because there is simply not enough data) and is uninterpretable.

An alternative approach is, therefore, to build a model that is **supported by the data** [(Bates et al., 2015)](https://arxiv.org/abs/1506.04967). This means developing a model, in which all variance components and correlation parameters are supported by the data. 

### Option 1 (transformed RTs)

In this webinar, we will start by fitting a varying intercepts and slopes model (without correlation) and then simplify it by assessing the random effects structure with the Principled Component Analysis (PCA). 

```{r}
mod1 <- lmer(transRT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + Bas.SemUnRel + SemRel.SemUnRel||Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel||TargetAnswer), dataCorr3)
```

Check the structure of the model:

```{r}
summary(rePCA(mod1))
VarCorr(mod1)
```

There is one component - the adjustment to the slope for `Bas.SemUnRel` - that does not explain any variance; it should be removed.

Note that, at this stage, we do not even need to check whether the model converged or not. And we certainly do not need to check the p-values.

```{r}
mod2 <- lmer(transRT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + SemRel.SemUnRel||Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel||TargetAnswer), dataCorr3)
```

Run PCA:

```{r}
summary(rePCA(mod2))
VarCorr(mod2)
```

Looks better, any convergence warnings?

```{r}
print(summary(mod2, corr = F))
```

Yes! `lmer()` and `glmer()` fits often produce convergence errors; however, they are very often false positives (type `?convergence` in the console to learn more). There are many things we can do to fix this.

We start by adjusting the convergence tolerance with the `optctrl` argument to `lmerControl`. My preferred optimiser is the bound optimization by quadratic approximation (Bobyqa) with a set of 200000 iterations (it is recommended to use at least 100000). Note that the choice of an optimiser can change the results of the model so it is recommended to fit models with different optimisers and then compare the outcomes. [Here](http://svmiller.com/blog/2018/06/mixed-effects-models-optimizer-checks/) is a useful and short blog post about it. 

```{r}
mod3 <- lmer(transRT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + SemRel.SemUnRel||Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel||TargetAnswer), dataCorr3,
             control = lmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e5)))

print(summary(mod3, corr = F))
```

One more thing to note here:

This model was fit using **REML (Restricted Maximum Likelihood Estimation)**, which is a way to estimate variance components. REML works by first getting regression residuals for the observations modeled by the fixed effects portion of the model. At this point, it ignores any variance components and estimates the statistical model for these residuals. Next, it uses **MLE (maximum likelihood estimation)** on the residuals to get estimates of the variance components. The main advantage of this approach is that the MLE adjusts the variance estimates for the fact that we are working with regression residuals ([here](http://users.stat.umn.edu/~gary/classes/5303/handouts/REML.pdf) is a super short paper/handout on this). Ben Bolker says that it's generally good to use REML, if it is available, when you are interested in the magnitude of the random effects variances, but never when you are comparing models with different fixed effects via hypothesis tests or information-theoretic criteria such as AIC.

Let's now check whether the residuals are normally disributed:

```{r}
qqnorm(resid(mod3))
plot(fitted(mod3), resid(mod3)) 
```

They do, and there isn't any evidence of heteroscedasticity in the residuals against fitted values plot.

Alright... It would seem that we cannot reject the null for the first contrast (semantically unrelated vs. unknown) but that we can for the second contrast (semantically related vs. unrelated).

**Confidence intervals**

```{r}
ConfidInt <- confint(mod3, parm = c("(Intercept)", "Bas.SemUnRel", "SemRel.SemUnRel"), method = "boot")
round(ConfidInt, 3)
```

**Plot of effects**

```{r}
# run this if the plot doesn't work on your machine:
Sys.setlocale(category = "LC_ALL", "English_United States.1252")

labels <- c("Contrast 1: German sem. unrelated vs. untrained pseudoword distractors", 
            "Contrast 2: German sem. related vs. unrelated distractors")

keep.terms <- names(fixef(mod3)[-1])

plot_model(mod3, title = "", terms = keep.terms, axis.labels = rev(labels), 
           type = "est", sort.est = NULL, colors = "bw", 
           show.values = TRUE, show.p = TRUE, value.offset = 0.4, 
           value.size = 4, dot.size = 2, line.size = 1, 
           vline.color = "black", 
           width = 0.1) + theme_sjplot2() +
           scale_color_sjplot(palette = "circus")
```

You can also extract AIC, BIC and log-likelihood from the model:

```{r}
glance(mod2)
```

Check out `tidy()` and `augment()` functions from the package `broom.mixed` for further options. If you need a Latex table with model output, the `stargazer()` function from the `stargazer` package will generate it for you.

**Table of effects**

```{r}
tab_model(mod2, 
          terms = keep.terms, 
          auto.label = FALSE, 
          pred.labels = labels, 
          show.se = TRUE, show.stat = TRUE, 
          show.ci = FALSE, string.se = "SE", 
          show.re.var = FALSE, 
          show.obs = FALSE, show.ngroups = FALSE,
          emph.p = FALSE, dv.labels = "Dependent Variable", show.icc = FALSE)
```

Note that the marginal $R^2$ shows the variance explained only by the fixed effects, while the conditional $R^2$ provides the variance explained by the entire model (including the random effects).

Note that because one of the failure to reject the null for the first contrast, a better model might be the one that only includes the second contrast:

```{r}
mod2b <- lmer(transRT ~ 1 + SemRel.SemUnRel +
                (1 + SemRel.SemUnRel||Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel||TargetAnswer), dataCorr3)

anova(mod2, mod2b)
```

You can also extract AIC, BIC and log-likelihood from the model:

```{r}
glance(mod2)
```

Check out `tidy()` and `augment()` functions from the package `broom.mixed` for further options.

### Option 2 (raw RTs)

GLMs (and GLMMs) are a class of fixed (and random) effects regression models that can be applied to several types of dependent variables (or distributions). There are three specifications in a GLM:

1. A linear predictor $\eta_{i}$ that has the form

$$
\eta_{i} = x^{'}_{i} \beta
$$
where $x_{i}$ is the vector of regressors for unit *i* with fixed effects $\beta$. 

2. A link function $g(\cdot)$ that coverts the expected value $\mu_{i}$ of the outcome variable $Y_{i}$ to the linear predictor $\eta_{i}$: 

$$
g(\mu_{i}) = \eta_{i}
$$
3. Variance in terms of the mean $\mu{i}$.

Thereby, the link function and the variance depend on the distribution of $Y_{i}$ (within the exponential family). For example, for a binary outcome, the logistic link function is used, for a count outcome, the log link function, and for the continuous outcome, the identity link function (note that these are the most common and not the only possible options).

In a GLMM, the linear predictor is a combination of the fixed and random effects excluding the residuals:

$$
\eta_{ij} = x^{'}_{ij} \beta + Z^{'}_{ij}u_{j}
$$

where $Z^{'}_{ij}$ is the random complement to $x^{'}_{ij}$ (or, the random effects design matrix) and $u_{j}$ is the the random complement to the fixed $\beta$ (or, the random effect).

As a starting point, we will fit a GLMM with a gamma distribution and the identity link (just a starting point, not necessarily the best option).

The choice of the gamma distribution is mainly motivated by considerations outlined in [Lo & Andrews (2015)](https://www.frontiersin.org/articles/10.3389/fpsyg.2015.01171/full). To summarise briefly:

* The Gamma distribution is the sum of multiple exponential distributions. Basically, it models the probability that no event occurs until a certain period of time. Therefore, it could be conceptualised as modelling several serial stages of processing, each of which finishes with a time that is exponentially distributed (see e.g., [Van Zandt & Ratcliff, 1995](https://link.springer.com/article/10.3758/BF03214411)).

* Because we assume that RT is a direct measure of the time required to name a picture, the function binding the expected outcome and the effect of the predictor is the identity link function.

```{r}
mod1_raw <- glmer(RT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + Bas.SemUnRel + SemRel.SemUnRel||Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel||TargetAnswer), dataCorr3, 
                family = Gamma(link = "identity"))

summary(rePCA(mod1_raw))
VarCorr(mod1_raw)
```

Simplify:

```{r}
mod2_raw <- glmer(RT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + SemRel.SemUnRel||Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel||TargetAnswer), dataCorr3, 
                family = Gamma(link = "identity"))

summary(rePCA(mod2_raw))
VarCorr(mod2_raw)
```

The model looks alright, but did it converge?

```{r}
print(summary(mod2_raw, corr = F))
```

No surprise here. `glmer()` fits often produce convergence errors; however, they are very often false positives (type `?convergence` in the console to learn more). There are many things we can do to fix this.

We start by adjusting the convergence tolerance with the `optctrl` argument to `glmerControl`. My preferred optimiser is the bound optimization by quadratic approximation (Bobyqa) with a set of 200000 iterations (it is recommended to use at least 100000).

```{r}
mod3_raw <- glmer(RT ~ Bas.SemUnRel + SemRel.SemUnRel +
                (1 + SemRel.SemUnRel||Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel||TargetAnswer), dataCorr3, 
                family = Gamma(link = "identity"),
                control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e5)))

print(summary(mod3_raw, corr = F))
```

Looks good! The problem is kind of solved (see [here](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html#reml-for-glmms) and/or type `?convergence` in the console to find more tips). But we still need to make sure that the model would perform similarly with other optimisers.

```{r}
fit_all <- lme4::allFit(mod2_raw, maxfun = 2e5)
ss <- summary(fit_all)
```

We can so check which optimisers worked and which not:

```{r}
ss$which.OK
```

And we can inspect warnings that these models produced:

```{r}
is.OK <- sapply(fit_all, is, "merMod")
fit_all.OK <- fit_all[is.OK]
lapply(fit_all.OK,function(x) x@optinfo$conv$lme4$messages)
```

We can also extract the estimates of the fixed effects and compare them:

```{r}
ss$fixef
```

Most importantly, we need to inspect the log-likelihoods for all models. If they are close to each other, we can conclude that the optimiser does not influence the parameter estimates.

```{r}
ss$llik
```

Let us now expect the model residuals:

```{r}
plot(residuals(mod3_raw))
plot(fitted(mod3_raw), resid(mod3_raw)) 
```

Looks okay, but it could be that the inverse link might provide a better fit. One could also examine other distributions (e.g., Inverse Gaussian).

**Table of effects**

```{r}
labels <- c("Contrast 1: German sem. unrelated vs. untrained pseudoword distractors", 
            "Contrast 2: German sem. related vs. unrelated distractors")

keep.terms <- names(fixef(mod3_raw)[-1])

tab_model(mod3_raw,
          transform = NULL,
          terms = keep.terms, 
          auto.label = FALSE, 
          pred.labels = labels, 
          show.se = TRUE, show.stat = TRUE, 
          show.ci = FALSE, string.se = "SE", 
          show.re.var = FALSE, 
          show.obs = FALSE, show.ngroups = FALSE,
          emph.p = FALSE, dv.labels = "Dependent Variable", show.icc = FALSE)
```

Is this model better than a model with only one contrast?

```{r}
mod3b_raw <- glmer(RT ~ 1 + SemRel.SemUnRel +
                (1 + SemRel.SemUnRel||Subj) + 
                (1 + Bas.SemUnRel + SemRel.SemUnRel||TargetAnswer), dataCorr3, 
                family = Gamma(link = "identity"),
                control = glmerControl(optimizer = "bobyqa", optCtrl = list(maxfun = 2e5)))

anova(mod3_raw, mod3b_raw)
```

No wonder given the value of the conditional $R^2$... The next step should probably be trying out the inverse link.

# Accuracy

 for accuracy https://stats.idre.ucla.edu/other/mult-pkg/introduction-to-generalized-linear-mixed-models/
